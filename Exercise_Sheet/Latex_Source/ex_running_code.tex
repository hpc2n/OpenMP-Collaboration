\subsection{Exercise: Compiling and Running OpenMP in an HPC environment }

Before compiling your code it is recommended to purge the module system:

\begin{verbatim}
ml purge
\end{verbatim}

Now, we can compile our code with the compiler we choose:

\subsubsection{GNU Compilers}

\begin{verbatim}
ml purge
module load GCC/11.3.0
# Fortran
gfortran -O3 -march=native -o code_serial_exe code.f90     #Serial code
gfortran -O3 -march=native -fopenmp -o code_exe code.f90   #OpenMP aware code

# C
gcc -O3 -march=native -o code_serial_exe code.c -lm        #Serial code
gcc -O3 -march=native -fopenmp -o code_exe code.c -lm      #OpenMP aware code
\end{verbatim}

\subsubsection{Intel Compilers}

\begin{verbatim}
ml purge
module load intel-compilers/2022.1.0 

# Fortran
ifort -O3 -march=core-avx2  -o code_serial_exe  code.f90     #Serial code
ifort -qopenmp -O3 -march=core-avx2 -o code_exe  code.f90   #OpenMP aware code

# C
icc -O3 -march=core-avx2 -o code_serial_exe  code.c -lm        #Serial code
icc -qopenmp -O3 -march=core-avx2 -o code_exe  code.c -lm      #OpenMP aware code
\end{verbatim}

Notice that we can compile the code with optimisation flags such as \verb+-O3 -march=native+ in case of a GCC compiler.  On COSMOS the standard \verb+-xHost+ of the Intel compiler will not result in efficient code, since COSMOS features AMD processors.   In this case it is typically advantageous to explicitly specify the instruction set of the deployed processor.  In case of COSMOS this is accomplished by using the option \verb+-march=core-avx2+.  
We can set the number of threads to  execute our code with an environment variable
for the OpenMP aware code:

\begin{verbatim}
export OMP_NUM_THREADS=4
./code_exe
\end{verbatim}

On HPC systems running the previous line on the login node will cause interference
with others' workflow. Thus, one should use the batch queue to run jobs. A typical
batch job looks like this:


\begin{verbatim}
#!/bin/bash
#SBATCH -A lu2023-7-61                  # you have one for this course
#SBATCH -t 00:03:00                     # time the code will take
#SBATCH -c 2                            # number of cpus requested
#SBATCH -J data_process                 # name of job
#SBATCH -o process_omp_%j.out           # output file
#SBATCH -e process_omp_%j.err           # error messages
#SBATCH --reservation=lu2023-7-61_dayX  # reservation change X for the day

cat $0                                  # it will log this script

ml purge  > /dev/null 2>&1 
module load GCC/11.3.0                      # choosing GNU compilers
#module load intel-compilers/2022.1.0       # choosing Intel compilers

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK # setting nr. threads

./code_exe
\end{verbatim}

In the present exercise, located in the folder:
\begin{verbatim}
   Templates/Day_1/Running_OpenMP/SampleScript
\end{verbatim}
you can find the {\verb+code.f90+ and \verb+code.c+  files that you can 
\begin{enumerate}
\item compile with the compiler of your  preference in serial and parallel (OpenMP) mode. 
\item submit the serial and parallel
executable codes to the SLURM queue by using the \verb+job.sh+ file which is also provided
in the folder (modify it to set the project ID, the number of cores you want, 
the time, and the reservation queue).
\item submit the job to the queue with the command: \verb+sbatch job.sh+, this will output
a number which is the "job ID"
\end{enumerate}
