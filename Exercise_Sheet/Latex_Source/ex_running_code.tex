\subsection{Exercise: Compiling and Running OpenMP in an HPC environment }

Before compiling your code it is recommended to purge the module system:

\begin{verbatim}
ml purge
\end{verbatim}

Now, we can compile our code with the compiler we choose:

\subsubsection{GNU Compilers}

\begin{verbatim}
ml purge
module load foss/2020b
# Fortran
gfortran -O3 -march=native -o code_serial_exe code.f90     #Serial code
gfortran -O3 -march=native -fopenmp -o code_exe code.f90   #OpenMP aware code

# C
gcc -O3 -march=native -o code_serial_exe code.c -lm        #Serial code
gcc -O3 -march=native -fopenmp -o code_exe code.c -lm      #OpenMP aware code
\end{verbatim}

\subsubsection{Intel Compilers}

\begin{verbatim}
ml purge
module load intel/2020a

# Fortran
ifort -O3 -xHost -o code_serial_exe  code.f90     #Serial code
ifort -qopenmp -O3 -xHost -o code_exe  code.f90   #OpenMP aware code

# C
icc -O3 -xHost -o code_serial_exe  code.c -lm        #Serial code
icc -qopenmp -O3 -xHost -o code_exe  code.c -lm      #OpenMP aware code
\end{verbatim}

Notice that we can compile the code with optimization flags such as "-O3 -march=native -xHost". 
We can set the number of threads to  execute our code with an environment variable
for the OpenMP aware code:

\begin{verbatim}
export OMP_NUM_THREADS=4
./code_exe
\end{verbatim}

On HPC systems running the previous line on the login node will cause interference
with others' workflow. Thus, one should use the batch queue to run jobs. A typical
batch job looks like this:


\begin{verbatim}
#!/bin/bash
#SBATCH -A  SNIC2021-22-37      # you have one for this course
#SBATCH -t 00:03:00             # time the code will take
#SBATCH -c 2                    # number of cpus requested
#SBATCH -J data_process         # name of job
#SBATCH -o process_omp_%j.out   # output file
#SBATCH -e process_omp_%j.err   # error messages
#SBATCH --reservation=snic2021-22-37-dayX  #reservation change X

cat $0                          # it will log this script

ml purge  > /dev/null 2>&1 
#module load foss/2020b         # choosing GNU compilers
#module load intel/2020a        # choosing Intel compilers

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK # setting nr. threads

./code_exe
\end{verbatim}

In the present exercise, located in the folder:
\begin{verbatim}
   Templates/Day_1/Running_OpenMP/SampleScript
\end{verbatim}
you can find the {\verb+code.f90+ and \verb+code.c+  files that you can 
\begin{enumerate}
\item compile with the compiler of your  preference in serial and parallel (OpenMP) mode. 
\item submit the serial and parallel
executable codes to the SLURM queue by using the \verb+job.sh+ file which is also provided
in the folder (modify it to set the project ID, the number of cores you want, 
the time, and the reservation queue).
\item submit the job to the queue with the command: \verb+sbatch job.sh+, this will output
a number which is the "job ID"
\item the command \verb+job-usage job_ID+ on the Kebnekaise terminal will give you an URL
which you can copy/paste on your local browser to monitor the CPU/Memory usage of your code
\end{enumerate}
