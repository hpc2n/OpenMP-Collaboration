\subsection{Important information: Compiling and Running OpenMP in an HPC environment }

Before compiling your code it is recommended to purge the module system:

\begin{verbatim}
ml purge
\end{verbatim}

Now we can compile our code with the compiler we choose:

\subsubsection{GNU Compilers}

\begin{verbatim}
module load foss/2020b
gfortran -fopenmp -o code_exe code.f90
\end{verbatim}

\subsubsection{Intel Compilers}

\begin{verbatim}
module load intel/2020a
ifort -qopenmp -O3 -xHost -o code_exe  code.f90
\end{verbatim}

Notice that we can compile the code with optimization flags such as "-O3 -xHost". 
We can set the number of threads to  execute our code with an environment variable:

\begin{verbatim}
export OMP_NUM_THREADS=4
./code_exe
\end{verbatim}

On HPC systems running the previous line on the login node will cause interference
with others' workflow. Thus, one should use the batch queue to run jobs. A typical
batch job looks like this:


\begin{verbatim}
#!/bin/bash
#SBATCH -A  SNIC-PROJECT        # you have one for this course
#SBATCH -t 00:03:00             # time the code will take
#SBATCH -c 2                    # number of cpus requested
#SBATCH -J data_process         # name of job
#SBATCH -o process_omp_%j.out   # output file
#SBATCH -e process_omp_%j.err   # error messages

cat $0                          # it will log this script

ml purge  > /dev/null 2>&1 
#module load foss/2020b         # choosing GNU compilers
#module load intel/2020a        # choosing Intel compilers

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK # setting nr. threads

./code_exe
\end{verbatim}

